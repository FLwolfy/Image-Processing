{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup Environment, Compile C++ Codes, and Import Dependencies**\n",
    "\n",
    "This assignment requires us to understand and apply traditional image processing techniques. Considering the efficiency of pixel traversal, I plan to use C++ as the main language and then package it into a Python package to provide an API for use in a Python notebook. I am using the third-party library pybind11 to package the C++ code. If the packaging doesn’t work, you can place the provided package in the modules folder; this package is simply a compilation of the C++ code under the scripts directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python compile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Processing**\n",
    "\n",
    "Before proceeding with image processing, I encapsulated an `Image` class to store information about the image, including its height, width, bytes per pixel, raw data, and so on. I then specified that the methods for image processing must be static and return a new image to ensure that the original image information remains unchanged. This principle will always apply: the image's data is read-only and cannot be modified.\n",
    "\n",
    "For detailed API information, please refer to the `README.md` file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **[Problem 1: Geometric Modification]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Special Effect via Compound Linear Geometric Modification**\n",
    "\n",
    "    Many times we will carry out basic geometric modification of the image, such as rotation, stretching, translation, below I use a special effect to show the effect of each geometric modification.\n",
    "\n",
    "    ## Assumptions:\n",
    "    - The original coordinates of a point are $(x, y)$.\n",
    "    - The center of the image is $(c_x, c_y)$.\n",
    "    - Time is $t$.\n",
    "    - Shrinkage rate is $s$ (percentage per second, expressed as a decimal).\n",
    "    - Rotation speed is $\\theta$ degrees per second (clockwise).\n",
    "    - Translation speed is $m$ pixels per second in the southeast direction.\n",
    "\n",
    "    ### 1. Translation (Southeast)\n",
    "    The point moves by $m \\cdot t$ pixels in both $x$ and $y$ directions:\n",
    "    $$\n",
    "    x_{\\text{trans}} = x + m \\cdot t\n",
    "    $$\n",
    "    $$\n",
    "    y_{\\text{trans}} = y + m \\cdot t\n",
    "    $$\n",
    "\n",
    "    ### 2. Scaling (Minification)\n",
    "    The point shrinks towards the center of the image with rate $s$:\n",
    "    $$\n",
    "    x_{\\text{scaled}} = c_x + (x - c_x) \\cdot (1 - s \\cdot t)\n",
    "    $$\n",
    "    $$\n",
    "    y_{\\text{scaled}} = c_y + (y - c_y) \\cdot (1 - s \\cdot t)\n",
    "    $$\n",
    "\n",
    "    ### 3. Rotation (Clockwise)\n",
    "    The point rotates around the center by $\\theta \\cdot t$ degrees:\n",
    "    - Convert degrees to radians:\n",
    "    $$\n",
    "    \\theta_{\\text{rad}} = \\frac{\\theta \\cdot t \\cdot \\pi}{180}\n",
    "    $$\n",
    "    - Apply the rotation matrix:\n",
    "    $$\n",
    "    x_{\\text{rotated}} = c_x + (x - c_x) \\cdot \\cos(\\theta_{\\text{rad}}) + (y - c_y) \\cdot \\sin(\\theta_{\\text{rad}})\n",
    "    $$\n",
    "    $$\n",
    "    y_{\\text{rotated}} = c_y - (x - c_x) \\cdot \\sin(\\theta_{\\text{rad}}) + (y - c_y) \\cdot \\cos(\\theta_{\\text{rad}})\n",
    "    $$\n",
    "\n",
    "    We can get the final result by bringing in the given values. The result is shown below. It is important to note that normal images may lose data after rotation because they are beyond the image boundaries, but I added padding to the images in order to preserve this rotated data, so in the following realistic images are not really relative sizes but stretched sizes, and the real sizes of each image I have labeled in parentheses. The end result is:\n",
    "    - At t=5 seconds: The image should shrink by 15%, rotate by 25 degrees, and \n",
    "    translate by 10 pixels.\n",
    "    - At t=20 seconds: The image should shrink by 60%, rotate by 100 degrees, \n",
    "    and translate by 40 pixels.\n",
    "\n",
    "    NOTES: To achieve the transition effect of non-complete pixels, I used the `bilinear interpolation` method to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special Effect via Compound Linear Geometric Modification\n",
    "\n",
    "barbara = Image(256, 256, 1)\n",
    "barbara.load(\"images/HW3/barbara.raw\")\n",
    "\n",
    "barbara_rotate = Image.rotate(barbara, 45, \"bilinear\")\n",
    "barbara_scale = Image.scale(barbara, 0.8, 0.6, \"bilinear\")\n",
    "barbara_translate = Image.translate(barbara, 50, 50, \"bilinear\")\n",
    "\n",
    "show_images([barbara, barbara_rotate, barbara_scale, barbara_translate], [f\"Original {barbara.raw_data.shape}\", f\"Rotate 45 {barbara_rotate.raw_data.shape}\", f\"Scale 0.8x0.6 {barbara_scale.raw_data.shape}\", f\"Translate 50, 50 {barbara_translate.raw_data.shape}\"], \"Barbara (Auto-Scaled)\")\n",
    "\n",
    "# Special Effect via Compound Linear Geometric Modification\n",
    "def special_effect(img: Image, s: float, theta: float, m: int) -> Image:\n",
    "    img = Image.scale(img, 1 - s, 1 - s, \"bilinear\")\n",
    "    img = Image.rotate(img, theta, \"bilinear\")\n",
    "    img = Image.translate(img, m, m, \"bilinear\")\n",
    "    \n",
    "    return img\n",
    "    \n",
    "velocities = {\n",
    "    \"s\": 0.03,\n",
    "    \"theta\": 5,\n",
    "    \"m\": 2\n",
    "}\n",
    "\n",
    "barbara_special_t5 = special_effect(barbara, velocities[\"s\"] * 5, velocities[\"theta\"] * 5, velocities[\"m\"] * 5)\n",
    "barbara_special_t20 = special_effect(barbara, velocities[\"s\"] * 20, velocities[\"theta\"] * 20, velocities[\"m\"] * 20)\n",
    "\n",
    "show_images([barbara, barbara_special_t5, barbara_special_t20], [f\"Original {barbara.raw_data.shape}\", f\"Special Effect t=5 {barbara_special_t5.raw_data.shape}\", f\"Special Effect t=20 {barbara_special_t20.raw_data.shape}\"], \"Barbara Special Effect (Auto-Scaled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Spatial Warping Techniques**\n",
    "\n",
    "    There are times when we will warp an image, from a simple realization of the effect, to some stretching at different angles. Let's say that the latest Apple products have cameras that allow you to get both a flat view and a top view from one camera, which uses image warping.\n",
    "\n",
    "    Let's say in the following case, we want the rectangular image to wrap into a circle, then it is very important that the pixels on the edges are the same, then we make a ratio based on the distance from the edge to the center of the circle, and then using this ratio, we make a linear CLAMP on each pixel point on the line, and finally we realize the WARPING from the rectangle to the circle.\n",
    "\n",
    "    ## Variables:\n",
    "    - Rectangular image dimensions: $w$ (width) and $h$ (height).\n",
    "    - Circular image parameters:\n",
    "    - Center at $(c_x, c_y)$.\n",
    "    - Radius $r = \\min\\left(\\frac{w}{2}, \\frac{h}{2}\\right)$.\n",
    "    - A pixel in the rectangle is located at $(x, y)$.\n",
    "    - A corresponding pixel in the circle will map to $(u, v)$.\n",
    "\n",
    "    ### 1. Define Polar Coordinates\n",
    "    We calculate the **polar coordinates** of a pixel $(x, y)$ in the rectangular image relative to its center $(c_x, c_y)$:\n",
    "\n",
    "    1. **Angle** $\\theta$ (azimuthal angle around the circle):\n",
    "    $$\n",
    "    \\theta = \\arctan2(y - c_y, x - c_x)\n",
    "    $$\n",
    "\n",
    "    2. **Distance from the center** $r_{\\text{rect}}$:\n",
    "    $$\n",
    "    r_{\\text{rect}} = \\sqrt{(x - c_x)^2 + (y - c_y)^2}\n",
    "    $$\n",
    "\n",
    "    ### 2. Map Distance to Circular Radius\n",
    "    To map the rectangular pixel to the circular domain, scale the distance $r_{\\text{rect}}$ to fit within the circle's radius $r$. This involves clamping $r_{\\text{rect}}$ to the maximum allowed value:\n",
    "    $$\n",
    "    r_{\\text{circle}} = r \\cdot \\text{CLAMP}\\left(\\frac{r_{\\text{rect}}}{r}, 0, 1\\right)\n",
    "    $$\n",
    "\n",
    "    ### 3. Compute Circular Coordinates\n",
    "    Using the scaled radius $r_{\\text{circle}}$ and the angle $\\theta$, compute the coordinates in the circular domain:\n",
    "    $$\n",
    "    u = c_x + r_{\\text{circle}} \\cdot \\cos(\\theta)\n",
    "    $$\n",
    "    $$\n",
    "    v = c_y + r_{\\text{circle}} \\cdot \\sin(\\theta)\n",
    "    $$\n",
    "\n",
    "    The detailed implementation effect is shown in the image below:\n",
    "    <div style=\"text-align: center;\">\n",
    "    <img src=\"images/circle_wraping.jpg\" alt=\"Implementation\" width=\"400\"/>\n",
    "    </div>\n",
    "\n",
    "    The same applies to the **inverse** from circle to rectangle, but due to the reduction of pixels, I use the “nearest” method to find the missing pixel information, so the final inverse is different from the original image, and by comparing the two images, we get an `MSE` of `82.79`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial warping techniques\n",
    "\n",
    "baboon = Image(512, 512, 1)\n",
    "baboon.load(\"images/HW3/baboon.raw\")\n",
    "\n",
    "baboon_warp = Image.circle_warp(baboon, False)\n",
    "baboon_warp_inverse = Image.circle_warp(baboon_warp, True)\n",
    "\n",
    "show_images([baboon, baboon_warp, baboon_warp_inverse], [f\"Original {baboon.raw_data.shape}\", f\"Warp {baboon_warp.raw_data.shape}\", f\"Inversed {baboon_warp_inverse.raw_data.shape}\"], \"Baboon (Auto-Scaled)\")\n",
    "print(\"The MSE between the original and inversed image is\", compare_images(baboon, baboon_warp_inverse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **[Problem 2: Texture Analysis and Segmentation]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Texture Classification**\n",
    "\n",
    "    There are all kinds of materials in life, plastic, wool, wood and so on. Each material has different characteristics. Identical or similar materials usually have similar characteristics. We can easily categorize some materials with the naked eye. However, considering computer image processing, it is not so simple. Here is my classification process for 15sample images with the following steps:\n",
    "\n",
    "    ### 1. Compute Law's Filters\n",
    "\n",
    "    Given two 3x1 vectors \n",
    "    $$\n",
    "    A = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix}, \\quad\n",
    "    B = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix},\n",
    "    $$\n",
    "    the outer product $ A \\times B^T $ is a 3x3 matrix computed as:\n",
    "\n",
    "    $$\n",
    "    A \\times B^T = \n",
    "    \\begin{bmatrix}\n",
    "    a_1 b_1 & a_1 b_2 & a_1 b_3 \\\\\n",
    "    a_2 b_1 & a_2 b_2 & a_2 b_3 \\\\\n",
    "    a_3 b_1 & a_3 b_2 & a_3 b_3\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    Similarly, for the vectors $ L_3 $, $ E_3 $, and $ S_3 $, we can compute the outer products:\n",
    "\n",
    "    $$\n",
    "    L_3 \\times E_3^T, \\quad L_3 \\times S_3^T, \\quad E_3 \\times S_3^T\n",
    "    $$\n",
    "\n",
    "    In each case, simply take the corresponding elements of the vectors and perform the multiplication as shown in the general outer product formula.\n",
    "\n",
    "    ### 2. Convolve and Get Features\n",
    "\n",
    "    By convolving all the law filters generated above and then taking its energy for each convolution result, we have condensed the result of each law filter for the current image into a feature, so that we obtain an array of features. By doing this for each image and then aggregating it with the kmeans method, we successfully classified 15 sample images in groups of 3.\n",
    "\n",
    "    Notes: To increase the recognition, I put extra min-max values as separate features for each image. meanwhile, for kmeans I used the open source library on github, here is the link: [https://github.com/marcoscastro/kmeans](https://github.com/marcoscastro/kmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texture Classification\n",
    "\n",
    "sample1 = Image(64, 64, 1)\n",
    "sample1.load(\"images/HW3/sample1.raw\")\n",
    "sample2 = Image(64, 64, 1)\n",
    "sample2.load(\"images/HW3/sample2.raw\")\n",
    "sample3 = Image(64, 64, 1)\n",
    "sample3.load(\"images/HW3/sample3.raw\")\n",
    "sample4 = Image(64, 64, 1)\n",
    "sample4.load(\"images/HW3/sample4.raw\")\n",
    "sample5 = Image(64, 64, 1)\n",
    "sample5.load(\"images/HW3/sample5.raw\")\n",
    "sample6 = Image(64, 64, 1)\n",
    "sample6.load(\"images/HW3/sample6.raw\")\n",
    "sample7 = Image(64, 64, 1)\n",
    "sample7.load(\"images/HW3/sample7.raw\")\n",
    "sample8 = Image(64, 64, 1)\n",
    "sample8.load(\"images/HW3/sample8.raw\")\n",
    "sample9 = Image(64, 64, 1)\n",
    "sample9.load(\"images/HW3/sample9.raw\")\n",
    "sample10 = Image(64, 64, 1)\n",
    "sample10.load(\"images/HW3/sample10.raw\")\n",
    "sample11 = Image(64, 64, 1)\n",
    "sample11.load(\"images/HW3/sample11.raw\")\n",
    "sample12 = Image(64, 64, 1)\n",
    "sample12.load(\"images/HW3/sample12.raw\")\n",
    "sample13 = Image(64, 64, 1)\n",
    "sample13.load(\"images/HW3/sample13.raw\")\n",
    "sample14 = Image(64, 64, 1)\n",
    "sample14.load(\"images/HW3/sample14.raw\")\n",
    "sample15 = Image(64, 64, 1)\n",
    "sample15.load(\"images/HW3/sample15.raw\")\n",
    "\n",
    "law_filter_size = 5\n",
    "num_of_clusters = 5\n",
    "max_iterations = 200\n",
    "\n",
    "# Cluster the samples\n",
    "samples = [sample1, sample2, sample3, sample4, sample5, sample6, sample7, sample8, sample9, sample10, sample11, sample12, sample13, sample14, sample15]\n",
    "cluster_results = Image.texture_cluster(samples, law_filter_size, num_of_clusters, max_iterations)\n",
    "\n",
    "# Show the classification\n",
    "classification = [[] for _ in range(num_of_clusters)]\n",
    "for i in range(len(samples)):  \n",
    "    for j in range(num_of_clusters):\n",
    "        if cluster_results[i] == j:\n",
    "            classification[j].append({f\"sample{i + 1}\" :samples[i]})\n",
    "             \n",
    "for cluster_id, images in enumerate(classification):\n",
    "    show_images([img for sample in images for img in sample.values()], \n",
    "                [img for sample in images for img in sample.keys()], \n",
    "                f\"Cluster {cluster_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Texture Segmentation 1**\n",
    "\n",
    "    With the above image classification, we can do the texture segmentation of the picture, the specific method is, we can divide the whole picture into some small patches, and then these patches are treated as a separate picture, and then this way we will get a lot of pictures, and then these pictures for texture classification, and then according to the obtained cluster group, to the color of the picture color.\n",
    "\n",
    "    $$\n",
    "    C(clusterID) = \\left\\lfloor \\frac{clusterID}{255} \\times (numOfClusters - 1) \\right\\rfloor\n",
    "    $$\n",
    "\n",
    "    Based on the final result on `Comb1` we find that it works relatively best when I take `patch_size = 16`, which preserves the number of patches and ensures that each patch has enough information. However, we found that the top-left, center, and top-right images segmented the best, but the bottom-left and bottom-right images are less able to distinguish clearly. **I think the reason: there are a lot of image junctions**, these patches will be difficult to be classified into a class, so they will confuse the overall classification structure, especially when kmeans converges, these patches are easy to be classified as a whole, so that the two images of the lower left and by the lower can not be well distinguished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texture Segmentation\n",
    "\n",
    "comb1 = Image(256, 256, 1)\n",
    "comb1.load(\"images/HW3/comb1.raw\")\n",
    "\n",
    "law_filter_size = 5\n",
    "num_of_clusters = 5\n",
    "patch_size = 16\n",
    "max_iterations = 200\n",
    "\n",
    "comb1_segmented = Image.texture_segment(comb1, 0, law_filter_size, patch_size, num_of_clusters, max_iterations)\n",
    "\n",
    "show_images([comb1, comb1_segmented], [f\"Original {comb1.raw_data.shape}\", f\"Segmented {comb1_segmented.raw_data.shape}\"], \"Comb1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Texture Segmentation 2**\n",
    "\n",
    "    According to the results of `Comb2`, the top left, top right and bottom right are classified successfully at `patch_size = 32`, but this classification effect is hard to distinguish the top left and bottom left materials, so much so that the results of my attempts from `patch_size 16-32` are not too good. I think there are the following reasons: \n",
    "    \n",
    "    1. the material of the upper left and lower left are very similar in itself, which indeed, if we people don't look carefully it's hard to distinguish the difference between these two, and their features are more similar, so much so that the result of obtaining ENERGY FEATURES with LAW FILTER is approximated; \n",
    "    \n",
    "    2. the similarity of these two materials is much greater than the similarity of the materials at the junction of the image, so much so that they The two materials are classified into a cluster (this is not reflected when `patch_size = 16 or 32`, because 16 or 32 can be divided by 256, but it is especially obvious when other patch_sizes are used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texture Segmentation\n",
    "\n",
    "comb2 = Image(256, 256, 1)\n",
    "comb2.load(\"images/HW3/comb2.raw\")\n",
    "\n",
    "law_filter_size = 5\n",
    "num_of_clusters = 4\n",
    "patch_size1 = 16\n",
    "patch_size2 = 32\n",
    "max_iterations = 200\n",
    "\n",
    "comb2_segmented1 = Image.texture_segment(comb2, 0, law_filter_size, patch_size1, num_of_clusters, max_iterations)\n",
    "comb2_segmented2 = Image.texture_segment(comb2, 1, law_filter_size, patch_size2, num_of_clusters, max_iterations)\n",
    "\n",
    "show_images([comb2, comb2_segmented1, comb2_segmented2], [f\"Original {comb2.raw_data.shape}\", f\"Segmented 16x16 {comb2_segmented1.raw_data.shape}\", f\"Segmented 32x32 {comb2_segmented2.raw_data.shape}\"], \"Comb2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **[Problem 3: Document Processing (Optical Character Recognition – OCR)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In life we are always inseparable from the character recognition, let's say the simplest pdf in the text extraction, many times we will use, but this contains the character recognition. The following is a very simple decision tree based character recognition method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Image Segmentations**\n",
    "\n",
    "    Often an image contains multiple independent characters, we need to extract these characters individually, which requires the use of image segmentation. The segmentation process uses a flood-fill method to identify and label connected regions in a binary image. The following is the Flood-Fill Algorithm that implements the image segementation.\n",
    "\n",
    "    For each pixel $ I(x, y) = 255 $ and $ L(x, y) = 0 $, perform a flood-fill:\n",
    "\n",
    "    - Initialize a queue:\n",
    "\n",
    "        $$ Q \\gets [(x, y)] $$\n",
    "\n",
    "    - While the queue is not empty:\n",
    "        - Pop a pixel $ (x', y') $ from $ Q $.\n",
    "        - Label the pixel:\n",
    "\n",
    "        $$ L(x', y') = \\text{label} $$\n",
    "\n",
    "        - Add its 4-connected neighbors $ (x_\\text{neighbor}, y_\\text{neighbor}) $ to $ Q $ if:\n",
    "\n",
    "        $$ \n",
    "        I(x_\\text{neighbor}, y_\\text{neighbor}) = 255 \n",
    "        \\quad \\text{and} \\quad \n",
    "        L(x_\\text{neighbor}, y_\\text{neighbor}) = 0 \n",
    "        $$\n",
    "    \n",
    "    We preserve the length and width of the segmented image in order to preserve its original aspect ratio in subsequent training. In order to clarify the part of the segmentation, we gray-scale the picture and then binarized, and then take the inverse color to ensure that the character parts are pure white 255, the blank part is pure black 0. The segmented subimages graphes are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Processing (Optical Character Recognition – OCR)\n",
    "\n",
    "training = Image(256, 256, 3)\n",
    "training.load(\"images/HW3/training.raw\")\n",
    "training = Image.gray_scale(training)\n",
    "training = Image.fixed_dither(training, 0, 128)\n",
    "training = Image.negative(training)\n",
    "\n",
    "training_shear1 = Image.shear(training, 0.15, 0, \"nearest\")\n",
    "training_shear2 = Image.shear(training, 0.3, 0, \"nearest\")\n",
    "\n",
    "training_dilate1 = Image.dilate(training, 0, 1)\n",
    "training_dilate2 = Image.dilate(training, 0, 2)\n",
    "\n",
    "training_minifized1 = Image.scale(training, 0.8, 0.8, \"nearest\")\n",
    "training_minimized2 = Image.scale(training, 0.5, 0.5, \"nearest\")\n",
    "\n",
    "training_shear_minifized = Image.shear(training_minifized1, 0.15, 0, \"nearest\")\n",
    "training_shear_dilate = Image.shear(training_dilate1, 0.15, 0, \"nearest\")\n",
    "training_minified_dilate = Image.scale(training_dilate1, 0.8, 0.8, \"nearest\")\n",
    "\n",
    "show_image(training, \"Training\")\n",
    "\n",
    "segments = Image.segment(training, 16)\n",
    "segments_shear1 = Image.segment(training_shear1, 16)\n",
    "segments_shear2 = Image.segment(training_shear2, 16)\n",
    "segments_dilate1 = Image.segment(training_dilate1, 16)\n",
    "segments_dilate2 = Image.segment(training_dilate2, 16)\n",
    "segments_minifized1 = Image.segment(training_minifized1, 16)\n",
    "segments_minimized2 = Image.segment(training_minimized2, 16)\n",
    "segments_shear_minifized = Image.segment(training_shear_minifized, 16)\n",
    "segments_shear_dilate = Image.segment(training_shear_dilate, 16)\n",
    "segments_minified_dilate = Image.segment(training_minified_dilate, 16)\n",
    "\n",
    "show_images(segments, [f\"Segment {i}\" for i in range(len(segments))], \"Training Segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Segments Feature Training**\n",
    "\n",
    "    After getting the segments, I extracted the features for each segment, I used the following features in total: `aspect_ratio`, `area_rate`, `perimeter_rate`, `euler_number`, `spatial_moment_00`, `spatial_moment_01`, `spatial_moment_10`, `centroid` , `symmetry`, `circularity` and their purpose is respectively:\n",
    "\n",
    "    1. **Aspect Ratio**: Measures the ratio of height to width, useful for distinguishing tall, narrow characters like '1' from wide ones like '0'.\n",
    "\n",
    "    2. **Area Rate**: The proportion of pixels in the segment compared to its bounding box, helpful for identifying densely packed characters versus sparse ones.\n",
    "\n",
    "    3. **Perimeter Rate**: Ratio of the perimeter to the bounding box dimensions, aiding in differentiating characters with complex outlines like '8' from simple ones like '1'.\n",
    "\n",
    "    4. **Euler Number**: The number of connected components minus the number of holes, critical for identifying characters with loops (e.g., '8' vs. '7').\n",
    "\n",
    "    5. **Spatial Moment $ M_{00} $**: Represents the total pixel intensity, useful for comparing the size and brightness of characters.\n",
    "\n",
    "    6. **Spatial Moment $ M_{01} $**: Indicates vertical distribution of pixel intensity, helping to locate the vertical balance of the character.\n",
    "\n",
    "    7. **Spatial Moment $ M_{10} $**: Indicates horizontal distribution of pixel intensity, helping to locate the horizontal balance of the character.\n",
    "\n",
    "    8. **Centroid**: The geometric center of the segment, useful for normalizing character positioning. It is essential to distinguish characters between '9' and '6'\n",
    "\n",
    "    9. **Symmetry**: Measures the reflectional symmetry of the character, important for distinguishing symmetrical characters like '8' from asymmetrical ones like '7'.\n",
    "\n",
    "    10. **Circularity**: The ratio of the square of the perimeter to the area, aiding in recognizing round characters like '0' versus angular ones like '3'.\n",
    "\n",
    "    After using Bitquads matrix alignment and window sliding to find the number of maps, it is easy to compute the above features, however, to make sure that the training set is sufficient, I `shear`, `diliate`, and `minify` the `training` images to make sure that some distortions are also possible. to ensure that some distorted images can also be recognized. I am using random forest as my decision tree forest model for training, the maximum depth parameter I provided 100, as below we can get the distribution of `feature importance`. We can find that the proportion related to spatial distribution is relatively high, because these features are the key to distinguish different characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def get_features(image: Image) -> list:\n",
    "    aspect_ratio = image.get_aspect_ratio()\n",
    "    area_rate = image.get_area_rate()\n",
    "    perimeter_rate = image.get_perimeter_rate()\n",
    "    euler_number = image.get_euler_number(True)\n",
    "    spatial_moment_00 = image.get_spatial_moment(0, 0)\n",
    "    spatial_moment_01 = image.get_spatial_moment(0, 1)\n",
    "    spatial_moment_10 = image.get_spatial_moment(1, 0)\n",
    "    centroid = image.get_centroid()\n",
    "    symmetry = image.get_symmetry()\n",
    "    circularity = image.get_circularity()\n",
    "    \n",
    "    return [aspect_ratio, area_rate, perimeter_rate, euler_number, spatial_moment_00, spatial_moment_01, spatial_moment_10, centroid[0], centroid[1] , symmetry, circularity]\n",
    "\n",
    "labels = np.array(['2', '3', '1', '4', '5', '6', '8', '7', '*', '9', '0', '.']) # Example labels\n",
    "\n",
    "# Extract features from segments using the function\n",
    "features = np.array([get_features(segment) for segment in segments])\n",
    "features_shear1 = np.array([get_features(segment) for segment in segments_shear1])\n",
    "features_shear2 = np.array([get_features(segment) for segment in segments_shear2])\n",
    "features_dilate1 = np.array([get_features(segment) for segment in segments_dilate1])\n",
    "features_dilate2 = np.array([get_features(segment) for segment in segments_dilate2])\n",
    "features_minifized1 = np.array([get_features(segment) for segment in segments_minifized1])\n",
    "features_maximized2 = np.array([get_features(segment) for segment in segments_minimized2])\n",
    "features_shear_minifized = np.array([get_features(segment) for segment in segments_shear_minifized])\n",
    "features_shear_dilate = np.array([get_features(segment) for segment in segments_shear_dilate])\n",
    "features_minified_dilate = np.array([get_features(segment) for segment in segments_minified_dilate])\n",
    "\n",
    "combined_features = np.concatenate((features, features_shear1, features_shear2, features_dilate1, features_dilate2, features_minifized1, features_maximized2, features_shear_minifized, features_shear_dilate, features_minified_dilate), axis=0)\n",
    "combined_labels = np.concatenate((labels, labels, labels, labels, labels, labels, labels, labels, labels, labels), axis=0)\n",
    "\n",
    "# Train decision tree classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=1000)\n",
    "clf.fit(combined_features, combined_labels)\n",
    "\n",
    "# Features Importance\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = [\"Aspect Ratio\", \"Area Rate\", \"Perimeter Rate\", \"Euler Number\", \"Spatial Moment 00\", \"Spatial Moment 01\", \"Spatial Moment 10\", \"Centroid X\", \"Centroid Y\", \"Symmetry\", \"Circularity\"]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(len(indices)):\n",
    "    print(f\"{f + 1}. {feature_names[indices[f]]} ({importances[indices[f]]})\")\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(len(importances)), importances[indices], align=\"center\")\n",
    "plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ORC Test and Validation 1**\n",
    "\n",
    "    Moving on to the testing session, I started with test set 1. The characters in this test set were all shear twisted, but because my training set included some shear processing, the results were relatively good, except for the character `'5'` which was recognized as `'2'`, the other characters were recognized correctly. The following are my illustrations: \n",
    "    \n",
    "    **REASON:** character '5' is recognized as '2' is that they are basically the same in terms of `perimeter`, `centroid`, and `aspect ratio`, which are the big weighted FEATURES, such that they are misrecognized in the final recognition. \n",
    "    \n",
    "    **POSSIBLE IMPROVEMENT:** add a feature that determines whether the opening is to the left or to the right, to distinguish 5 from 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = Image(256, 256, 3)\n",
    "test1.load(\"images/HW3/test1.raw\")\n",
    "test1 = Image.gray_scale(test1)\n",
    "test1 = Image.fixed_dither(test1, 0, 128)\n",
    "test1 = Image.negative(test1)\n",
    "\n",
    "show_image(test1, \"Test1\")\n",
    "\n",
    "segments1 = Image.segment(test1, 16)\n",
    "show_images(segments1, [f\"Segment {i}\" for i in range(len(segments1))], \"Test1 Segments\")\n",
    "\n",
    "# Extract features from segments using the function\n",
    "features = [get_features(segment) for segment in segments1]\n",
    "features = np.array(features)\n",
    "\n",
    "clf.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ORC Test and Validation 2**\n",
    "\n",
    "    Next is the second test set, which has far more transformations than the first, but because my training set contains diliate and shear, some of the 'slanting' and 'fattening' character transformations are able to be handled. However, this time the recognition went wrong wrongly by recognizing '*' as '0' as well as recognizing '8' as '7' I think the problem is as follows:\n",
    "    \n",
    "    1. **'*' recognized as '0'**: because the resolution of the picture is very small, resulting in * itself, the `perimeter rate`, `centroid`, `aspect ratio` and 0 similar, although their Euler's number is not the same, due to the image transformation of this time the drastic, did not recognize successfully.\n",
    "    2. **'8' was recognized as '7'**: which I think was completely distorted due to the fact that the 8 in the original image was very small, and I didn't have a small enough 8 in my training set, resulting in inaccurate pixel accuracy.\n",
    "\n",
    "    Both of the above points are due to the accuracy problem caused by the **image being too small**, I think the solution is not only to increase the training set of small resolution images, but also to add some size-independent features such as skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = Image(256, 256, 1)\n",
    "test2.load(\"images/HW3/test2.raw\")\n",
    "test2 = Image.gray_scale(test2)\n",
    "test2 = Image.fixed_dither(test2, 0, 128)\n",
    "test2 = Image.negative(test2)\n",
    "\n",
    "show_image(test2, \"Test2\")\n",
    "\n",
    "segments2 = Image.segment(test2, 16)\n",
    "show_images(segments2, [f\"{i}\" for i in range(len(segments2))], \"Test1 Segments\")\n",
    "\n",
    "# Extract features from segments using the function\n",
    "features = [get_features(segment) for segment in segments2]\n",
    "features = np.array(features)\n",
    "\n",
    "clf.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ORC Think Beyond**\n",
    "\n",
    "    After running through the recognition of the numbers, we can try to recognize some of the letters, even though the current letters are not present in the training set, but we can discuss what FEATURES are actually dominating by analyzing why these letters are recognized as these numbers:\n",
    "\n",
    "    1. **(Top part of the letter i) → .**:\n",
    "    The top part of i resembles a small dot, closely matching the shape and features of . such as `aspect_ratio` and `area_rate`.\n",
    "\n",
    "    2. **h → 2, k → 2**:\n",
    "    The shapes of h and k have vertical strokes and curves, which partially overlap with the features of 2, particularly `circularity` and `symmetry`.\n",
    "\n",
    "    3. **T → 7**:\n",
    "    Both T and 7 share a horizontal line and vertical structure, with similar `aspect_ratio` and `perimeter_rate`, leading to this misclassification.\n",
    "\n",
    "    4. **n → 4 and \\***:\n",
    "    The curve of n resembles part of 4, and its overall structure might align with the star shape * based on `spatial distribution`.\n",
    "\n",
    "    5. **(Bottom part of the letter i) → 1**:\n",
    "    The bottom of i is a straight vertical stroke, similar to 1, resulting in a match based on `aspect_ratio`.\n",
    "\n",
    "    6. **d → 6**:\n",
    "    The round part of d corresponds to the circular feature of 6, and their `circularity` and `symmetry` align closely.\n",
    "\n",
    "    7. **B → 8**:\n",
    "    Both B and 8 have a double-loop structure, making their `circularity` and `spatial moments` closely match.\n",
    "\n",
    "    8. **e → 4, o → 4**:\n",
    "    The curved shapes of e and o share geometric similarities with 4, especially in terms of `circularity` and `aspect_ratio`.\n",
    "\n",
    "    9. **y → 7**:\n",
    "    The structure of y, with a vertical stroke and downward curve, matches well with 7 in `aspect_ratio` and `symmetry`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = Image(256, 256, 3)\n",
    "test3.load(\"images/HW3/test3.raw\")\n",
    "test3 = Image.gray_scale(test3)\n",
    "test3 = Image.fixed_dither(test3, 0, 128)\n",
    "test3 = Image.negative(test3)\n",
    "\n",
    "show_image(test3, \"Test3\")\n",
    "\n",
    "segments3 = Image.segment(test3, 16)\n",
    "show_images(segments3, [f\"{i}\" for i in range(len(segments3))], \"Test3 Segments\")\n",
    "\n",
    "# Extract features from segments using the function\n",
    "features = [get_features(segment) for segment in segments3]\n",
    "features = np.array(features)\n",
    "\n",
    "clf.predict(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
